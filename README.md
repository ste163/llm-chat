# llm-chat

Ideally, a locally running llm chat just like chatgpt

TODO:

- Get an llm running locally
- Send a message from the frontend and have it return response from backend
- A "clean" UI
- Saved messages
