# llm-chat

Ideally, a locally running llm chat just like chatgpt

TODO:

- Attempt to use [mistral.rs](https://github.com/EricLBuehler/mistral.rs) and a model file to get it running. If that works, try to allow for a download to the file through the tauri app.
- Send a message from the frontend and have it return response from backend
- A "clean" UI
- Saved messages
